## Introduction
è¿™é¡¹å·¥ä½œæ˜¯åœ¨Tevalçš„ä»£ç æ¡†æ¶çš„åŸºç¡€ä¸Šæ„å»ºçš„é‡‘èbenchmark. å’ŒåŸç‰ˆçš„Tevalä¸€æ ·ï¼Œæˆ‘ä»¬è¯„ä¼°æ¨¡å‹åœ¨ä»¥ä¸‹å…­ä¸ªç»´åº¦çš„èƒ½åŠ›: instruct, plan, reason, retrieve, understand, ä»¥åŠ review.
## ğŸ› ï¸ Preparations

```bash
$ git clone https://github.com/Jz95585/T-Eval.git
$ cd T-Eval
$ pip install -r requirements.txt
$ cd lagent && pip install -e .
```

##  ğŸ›«ï¸ Get Started

### ğŸ¤– API Models

1. è®¾ç½®OPENAI_API_KEYå’ŒOPENAI_API_BASE
```bash
export OPENAI_API_KEY=xxxxxxxxx
export OPENAI_API_BASE=xxxxxxxxx
```
2. ä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿è¡Œè¯„æµ‹(model_name å¯ä»¥æ˜¯OpenAIæ¨¡å‹çš„åå­—æˆ–è€…æ”¯æŒOpenAIæ–¹å¼è°ƒç”¨çš„æ¨¡å‹ï¼Œä¾‹å¦‚deepseek-chat)
<!-- ```bash
# test all data at once
sh test_all_en.sh api gpt-4-1106-preview gpt4
# test ZH dataset
sh test_all_zh.sh api gpt-4-1106-preview gpt4
# test for Instruct only
python test.py --model_type api --model_path gpt-4-1106-preview --resume --out_name instruct_gpt4.json --out_dir work_dirs/gpt4/ --dataset_path data/instruct_v2.json --eval instruct --prompt_type json
``` -->
```bash
sh test.sh model_name
```

### ğŸ¤— HuggingFace Models

1. ä¸‹è½½HuggingFaceæ¨¡å‹åˆ°ä½ çš„æœ¬åœ°è·¯å¾„.
<!-- 2. Modify the `meta_template` json according to your tested model. -->
2. ä½¿ç”¨vllméƒ¨ç½²ä½ çš„æ¨¡å‹
```bash
CUDA_VISIBLE_DEVICES=1,2,3,4 python -m vllm.entrypoints.openai.api_server \
    --model model_local_path \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.9 \
    --served-model-name model_name \
    --block-size 16  \
    --trust-remote-code \
    --port 8081
```
3. ä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿è¡Œè¯„æµ‹
```bash
export MKL_THREADING_LAYER=GNU
export MKL_SERVICE_FORCE_INTEL=1
export OPENAI_API_KEY="EMPTY"
export OPENAI_API_BASE=http://0.0.0.0:8081/v1

sh test.sh model_name
```
<!-- ```bash
# test all data at once
sh test_all_en.sh hf $HF_PATH $HF_MODEL_NAME $META_TEMPLATE
# test ZH dataset
sh test_all_zh.sh hf $HF_PATH $HF_MODEL_NAME $META_TEMPLATE
# test for Instruct only
python test.py --model_type hf --model_path $HF_PATH --resume --out_name instruct_$HF_MODEL_NAME.json --out_dir data/work_dirs/ --dataset_path data/instruct_v1.json --eval instruct --prompt_type json --model_display_name $HF_MODEL_NAME --meta_template $META_TEMPLATE
``` -->

### ğŸ’« Final Results
ä¸€æ—¦ä½ æµ‹è¯•å®Œäº†æ‰€æœ‰çš„æ•°æ®ï¼Œç»“æœçš„ç»†èŠ‚ä¼šæ”¾åœ¨ `out_dirs/model_name/model_name_-1_zh.json`  é€šè¿‡ä»¥ä¸‹å‘½ä»¤è®¡ç®—æœ€ç»ˆåˆ†æ•°:
```bash
python teval/utils/convert_results.py --result_path out_dirs/model_name/model_name_-1_zh.json
```

## ğŸ“Š Benchmark Results

<!-- More detailed and comprehensive benchmark results can refer to ğŸ† [T-Eval official leaderboard](https://open-compass.github.io/T-Eval/leaderboard.html) ! -->

<!-- <div>
<center>
<img src="figs/teval_results.png">
</div> -->

### âœ‰ï¸ Submit Your Results

<!-- You can submit your inference results (via running test.py) to this [email](lovesnow@mail.ustc.edu.cn). We will run your predictions and update the results in our leaderboard. Please also provide the scale of your tested model. A sample structure of your submission should be like:
```
$model_display_name/
    instruct_$model_display_name/
        query_0_1_0.json
        query_0_1_1.json
        ...
    plan_json_$model_display_name/
    plan_str_$model_display_name/
    ...
``` -->

<!-- ## ğŸ’³ License

This project is released under the Apache 2.0 [license](./LICENSE). -->
